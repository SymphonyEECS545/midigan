{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbnahata/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import SharedArray as sa\n",
    "from sklearn.utils import shuffle\n",
    "from utils import *\n",
    "from superGAN_ops import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MidiNet(object):\n",
    "    def __init__ (self, sess, \n",
    "                  batch_size=72, output_w=16, output_h=128,\n",
    "                  gf_dim = 64, df_dim = 64, sample_size=72,\n",
    "                  y_dim=13, c_dim=1, z_dim = 100):\n",
    "        self.sess = sess\n",
    "        self.is_grayscale = (c_dim == 1)\n",
    "        self.batch_size = batch_size\n",
    "        self.output_w = output_w\n",
    "        self.output_h = output_h\n",
    "        \n",
    "        self.gf_dim = gf_dim\n",
    "        self.df_dim = df_dim\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        self.y_dim = y_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.c_dim = c_dim\n",
    "        \n",
    "        #batch normalization: deals with poor initialization and helps gradient flow\n",
    "        self.d_bn0 = batch_norm(name='d_bn0')\n",
    "        self.d_bn1 = batch_norm(name='d_bn1')\n",
    "        self.d_bn2 = batch_norm(name='d_bn2')\n",
    "        self.d_bn3 = batch_norm(name='d_bn3')\n",
    "        \n",
    "        self.g_prev_bn0 = batch_norm(name='g_prev_bn0')\n",
    "        self.g_prev_bn1 = batch_norm(name='g_prev_bn1')\n",
    "        self.g_prev_bn2 = batch_norm(name='g_prev_bn2')\n",
    "        self.g_prev_bn3 = batch_norm(name='g_prev_bn3')\n",
    "        \n",
    "        self.g_bn0 = batch_norm(name='g_bn0')\n",
    "        self.g_bn1 = batch_norm(name='g_bn1')\n",
    "        self.g_bn2 = batch_norm(name='g_bn2')\n",
    "        self.g_bn3 = batch_norm(name='g_bn3')\n",
    "        self.g_bn4 = batch_norm(name='g_bn4')\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.y = tf.placeholder(tf.float32,[self.batch_size, self.y_dim], name='y')\n",
    "        self.prev_bar = tf.placeholder(tf.float32, [self.batch_size] + [self.output_w, self.output_h, self.c_dim],\n",
    "                                      name='prev_bar')\n",
    "        self.images = tf.placeholder(tf.float32, [self.batch_size] + [self.output_w, self.output_h, self.c_dim],\n",
    "                                    name='real_images')\n",
    "        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')\n",
    "        \n",
    "        self.G = self.generator(self.z, self.y, self.prev_bar)\n",
    "        self.D, self.D_logits, self.fm = self.discriminator(self.images, self.y, reuse=False)\n",
    "        self.sampler = self.sampler(self.z, self.y, self.prev_bar)\n",
    "        \n",
    "        self.D_, self.D_logits_ , self.fm_ = self.discriminator(self.G, self.y, reuse=True)\n",
    "    \n",
    "        self.z_sum = tf.summary.histogram(\"z\", self.z)\n",
    "        \n",
    "        self.d_sum = tf.summary.histogram(\"d\", self.D)\n",
    "        self.d__sum = tf.summary.histogram(\"d_\", self.D_)\n",
    "        self.G_sum = tf.summary.image(\"G\", self.G)\n",
    "        \n",
    "        self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = self.D_logits,\n",
    "                                                                                  labels = 0.9*tf.ones_like(self.D)))\n",
    "        self.d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = self.D_logits_,\n",
    "                                                                                  labels = tf.zeros_like(self.D_)))\n",
    "        self.g_loss0 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = self.D_logits_, \n",
    "                                                                              labels = tf.ones_like(self.D_)))\n",
    "\n",
    "\n",
    "        #Feature Matching\n",
    "        self.features_from_g = tf.reduce_mean(self.fm_, reduction_indices=(0))\n",
    "        self.features_from_i = tf.reduce_mean(self.fm, reduction_indices=(0))\n",
    "        self.fm_g_loss1 =tf.multiply(tf.nn.l2_loss(self.features_from_g - self.features_from_i), 0.1)\n",
    "\n",
    "        self.mean_image_from_g = tf.reduce_mean(self.G, reduction_indices=(0))\n",
    "        self.mean_image_from_i = tf.reduce_mean(self.images, reduction_indices=(0))\n",
    "        self.fm_g_loss2 = tf.multiply(tf.nn.l2_loss(self.mean_image_from_g - self.mean_image_from_i), 0.01)\n",
    "\n",
    "\n",
    "        self.d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", self.d_loss_real)\n",
    "        self.d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", self.d_loss_fake)\n",
    "\n",
    "        self.d_loss = self.d_loss_real + self.d_loss_fake\n",
    "        self.g_loss = self.g_loss0 + self.fm_g_loss1 + self.fm_g_loss2\n",
    "\n",
    "        self.g_loss_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
    "        self.d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
    "\n",
    "        t_vars = tf.trainable_variables()\n",
    "\n",
    "        self.d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "        self.g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def train(self, config):\n",
    "    \n",
    "#         if config.dataset == 'MidiNet_v1':\n",
    "            # change the file path to your dataset\n",
    "        print(\"Begin training...\")\n",
    "        data_X = np.load('/Users/sbnahata/Documents/University of Michigan/EECS 545/Project_local/data_x.npy')\n",
    "        data_X = np.reshape(data_X, [128, 16, -1, 1])\n",
    "        data_X = np.transpose(data_X,(2,1,0,3))\n",
    "        \n",
    "        prev_X = np.load('/Users/sbnahata/Documents/University of Michigan/EECS 545/Project_local/prev_x.npy')\n",
    "        prev_X = np.reshape(prev_X, [128, 16, -1, 1])\n",
    "        prev_X = np.transpose(prev_X,(2,1,0,3))\n",
    "        \n",
    "        data_y = np.load('/Users/sbnahata/Documents/University of Michigan/EECS 545/Project_local/data_y.npy')\n",
    "        data_y = np.transpose(data_y,(1,0))\n",
    "\n",
    "#             data_X, prev_X, data_y = shuffle(data_X,prev_X,data_y, random_state=0)\n",
    "            \n",
    "#             data_X = np.transpose(data_X,(0,2,3,1))\n",
    "#             prev_X = np.transpose(prev_X,(0,2,3,1))\n",
    "#             print (prev_X.shape)\n",
    "        \n",
    "        d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n",
    "                          .minimize(self.d_loss, var_list=self.d_vars)\n",
    "        g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n",
    "                          .minimize(self.g_loss, var_list=self.g_vars)\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        self.g_sum = tf.summary.merge([self.z_sum, self.d__sum, \n",
    "            self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])\n",
    "        self.d_sum = tf.summary.merge([self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])\n",
    "        self.writer = tf.summary.FileWriter(\"./logs\", self.sess.graph)\n",
    "\n",
    "        sample_z = np.random.normal(0, 1, size=(self.sample_size , self.z_dim))\n",
    "        sample_files = data_X[0:self.sample_size]\n",
    "        \n",
    "        save_images(data_X[np.arange(len(data_X))[:5]]*1, [1, 5],\n",
    "        './{}/Train.png'.format(config.sample_dir))\n",
    "        \n",
    "        \n",
    "        sample_images = data_X[0:self.sample_size]\n",
    "        counter = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "#         if self.load(self.checkpoint_dir):\n",
    "#             print(\" [*] Load SUCCESS\")\n",
    "#         else:\n",
    "#             print(\" [!] Load failed...\")\n",
    "        \n",
    "\n",
    "        sample_labels = sloppy_sample_labels()\n",
    "        print(\"Starting epochs...\")\n",
    "        for epoch in xrange(config.epoch):\n",
    "            \n",
    "            batch_idxs = len(data_X) // config.batch_size\n",
    "            \n",
    "            \n",
    "\n",
    "            for idx in xrange(0, batch_idxs):\n",
    "                \n",
    "                batch_images = data_X[idx*config.batch_size:(idx+1)*config.batch_size]\n",
    "                prev_batch_images = prev_X[idx*config.batch_size:(idx+1)*config.batch_size]\n",
    "                \n",
    "                batch_labels = data_y[idx*config.batch_size:(idx+1)*config.batch_size]\n",
    "                '''\n",
    "                Note that the mu and sigma are set to (-1,1) in the experiment of the paper :\n",
    "                \"MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation\"\n",
    "                However, the result are similar by using (0,1)\n",
    "                '''\n",
    "                batch_z = np.random.normal(0, 1, [config.batch_size, self.z_dim]) \\\n",
    "                            .astype(np.float32)\n",
    "\n",
    "                \n",
    "                # Update D network\n",
    "                _, summary_str = self.sess.run([d_optim, self.d_sum],\n",
    "                    feed_dict={ self.images: batch_images, self.z: batch_z ,self.y:batch_labels, self.prev_bar:prev_batch_images })\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                # Update G network\n",
    "                _, summary_str = self.sess.run([g_optim, self.g_sum],\n",
    "                        feed_dict={ self.images: batch_images, self.z: batch_z ,self.y:batch_labels, self.prev_bar:prev_batch_images })\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n",
    "                # We've tried to run more d_optim and g_optim, while getting a better result by running g_optim twice in this MidiNet version.\n",
    "                _, summary_str = self.sess.run([g_optim, self.g_sum],\n",
    "                        feed_dict={ self.images: batch_images, self.z: batch_z ,self.y:batch_labels, self.prev_bar:prev_batch_images })\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "                    \n",
    "                errD_fake = self.d_loss_fake.eval({self.z: batch_z, self.y:batch_labels, self.prev_bar:prev_batch_images })\n",
    "                errD_real = self.d_loss_real.eval({self.images: batch_images, self.y:batch_labels })\n",
    "                errG = self.g_loss.eval({self.images: batch_images, self.z: batch_z, self.y:batch_labels, self.prev_bar:prev_batch_images })\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\" \\\n",
    "                    % (epoch, idx, batch_idxs,\n",
    "                        time.time() - start_time, errD_fake+errD_real, errG))\n",
    "\n",
    "                if np.mod(counter, 100) == 1:\n",
    "                    \n",
    "                    samples, d_loss, g_loss = self.sess.run(\n",
    "                        [self.sampler, self.d_loss, self.g_loss],\n",
    "                        feed_dict={self.z: sample_z, self.images: sample_images, self.y:sample_labels, self.prev_bar:prev_batch_images }\n",
    "                    )\n",
    "                    #samples = (samples+1.)/2.\n",
    "                    save_images(samples[:5,:], [1, 5],\n",
    "                                './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx))\n",
    "                    print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss))\n",
    "\n",
    "                    np.save('./{}/train_{:02d}_{:04d}'.format(config.gen_dir,  epoch, idx), samples)\n",
    "\n",
    "                if np.mod(counter, len(data_X)//config.batch_size) == 0:\n",
    "                    print(\"Saving Checkpoint\")\n",
    "                    self.save(config.checkpoint_dir, counter)\n",
    "            print(\"Epoch: [%2d] time: %4.4f, d_loss: %.8f\" \\\n",
    "            % (epoch, \n",
    "                time.time() - start_time, (errD_fake+errD_real)/batch_idxs))\n",
    "    \n",
    "    def generator(self, z, y, prev_x=None):\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            print(\"GENERATOR\")\n",
    "            h0_prev = lrelu(self.g_prev_bn0(conv2d(prev_x, 16, k_h=1, k_w=128,d_h=1, d_w=2, name='g_h0_prev_conv')))\n",
    "            h1_prev = lrelu(self.g_prev_bn1(conv2d(h0_prev, 16, k_h=2, k_w=1, name='g_h1_prev_conv')))\n",
    "            h2_prev = lrelu(self.g_prev_bn2(conv2d(h1_prev, 16, k_h=2, k_w=1, name='g_h2_prev_conv')))\n",
    "            h3_prev = lrelu(self.g_prev_bn3(conv2d(h2_prev, 16, k_h=2, k_w=1, name='g_h3_prev_conv')))\n",
    "\n",
    "            yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n",
    "            z = tf.concat([z, y], 1)\n",
    "\n",
    "            h0 = tf.nn.relu(self.g_bn0(linear(z, 1024, 'g_h0_lin')))\n",
    "            h0 = tf.concat([h0, y], 1)\n",
    "\n",
    "            h1 = tf.nn.relu(self.g_bn1(linear(h0, self.gf_dim*2*2*1, 'g_h1_lin')))\n",
    "\n",
    "            h1 = tf.reshape(h1, [self.batch_size, 2, 1, self.gf_dim * 2])\n",
    "            h1 = conv_cond_concat(h1, yb)\n",
    "            h1 = conv_prev_concat(h1, h3_prev)\n",
    "\n",
    "            h2 = tf.nn.relu(self.g_bn2(deconv2d(h1, [self.batch_size, 4, 1, self.gf_dim * 2],\n",
    "                                                k_h=2, k_w=1,d_h=2, d_w=2 ,name='g_h2')))\n",
    "            h2 = conv_cond_concat(h2, yb)\n",
    "            h2 = conv_prev_concat(h2, h2_prev)\n",
    "\n",
    "            h3 = tf.nn.relu(self.g_bn3(deconv2d(h2, [self.batch_size, 8, 1, self.gf_dim * 2],k_h=2, k_w=1,d_h=2, d_w=2 ,name='g_h3')))\n",
    "            h3 = conv_cond_concat(h3, yb)\n",
    "            h3 = conv_prev_concat(h3, h1_prev)\n",
    "\n",
    "            h4 = tf.nn.relu(self.g_bn4(deconv2d(h3, [self.batch_size, 16, 1, self.gf_dim * 2],k_h=2, k_w=1,d_h=2, d_w=2 ,name='g_h4')))\n",
    "            h4 = conv_cond_concat(h4, yb)\n",
    "            h4 = conv_prev_concat(h4, h0_prev)\n",
    "\n",
    "            return tf.nn.sigmoid(deconv2d(h4, [self.batch_size, 16, 128, self.c_dim],k_h=1, k_w=128,d_h=1, d_w=2, name='g_h5'))\n",
    "\n",
    "    def discriminator(self, x, y=None, reuse=False):\n",
    "        with tf.variable_scope(\"discriminator\") as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "            df_dim = 64\n",
    "            dfc_dim = 1024\n",
    "\n",
    "            print(\"DISCRIMINATOR\")\n",
    "            yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n",
    "\n",
    "            x = conv_cond_concat(x, yb)\n",
    "\n",
    "            h0 = lrelu(conv2d(x, self.c_dim + self.y_dim,k_h=2, k_w=128, name='d_h0_conv'))\n",
    "            fm = h0\n",
    "            h0 = conv_cond_concat(h0, yb)\n",
    "\n",
    "            h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim,k_h=4, k_w=1, name='d_h1_conv')))\n",
    "            h1 = tf.reshape(h1, [self.batch_size, -1])    \n",
    "            h1 = tf.concat([h1, y],1)\n",
    "\n",
    "            h2 = lrelu(self.d_bn2(linear(h1, dfc_dim, 'd_h2_lin')))\n",
    "            h2 = tf.concat([h2, y],1)\n",
    "\n",
    "            h3 = linear(h2, 1, 'd_h3_lin')\n",
    "\n",
    "            return tf.nn.sigmoid(h3), h3, fm\n",
    "    \n",
    "    def sampler(self, z, y=None, prev_x=None):\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            scope.reuse_variables()\n",
    "            print(\"SAMPLER\")\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            h0_prev = lrelu(self.g_prev_bn0(conv2d(prev_x, 16, k_h=1, k_w=128, d_h=1, d_w=2,name='g_h0_prev_conv')))\n",
    "            print(h0_prev)\n",
    "            h1_prev = lrelu(self.g_prev_bn1(conv2d(h0_prev, 16, k_h=2, k_w=1, name='g_h1_prev_conv')))\n",
    "            h2_prev = lrelu(self.g_prev_bn2(conv2d(h1_prev, 16, k_h=2, k_w=1, name='g_h2_prev_conv')))\n",
    "            h3_prev = lrelu(self.g_prev_bn3(conv2d(h2_prev, 16, k_h=2, k_w=1, name='g_h3_prev_conv')))\n",
    "\n",
    "\n",
    "            yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n",
    "            z = tf.concat([z, y],1)\n",
    "\n",
    "            h0 = tf.nn.relu(self.g_bn0(linear(z, 1024, 'g_h0_lin')))\n",
    "            h0 = tf.concat([h0, y],1)\n",
    "\n",
    "            h1 = tf.nn.relu(self.g_bn1(linear(h0, self.gf_dim*2*2*1, 'g_h1_lin')))\n",
    "\n",
    "            h1 = tf.reshape(h1, [self.batch_size, 2, 1, self.gf_dim * 2])\n",
    "            h1 = conv_cond_concat(h1, yb)\n",
    "            h1 = conv_prev_concat(h1, h3_prev)\n",
    "\n",
    "            h2 = tf.nn.relu(self.g_bn2(deconv2d(h1, [self.batch_size, 4, 1, self.gf_dim * 2],k_h=2, k_w=1,d_h=2, d_w=2 ,name='g_h2')))\n",
    "            h2 = conv_cond_concat(h2, yb)\n",
    "            h2 = conv_prev_concat(h2, h2_prev)\n",
    "\n",
    "            h3 = tf.nn.relu(self.g_bn3(deconv2d(h2, [self.batch_size, 8, 1, self.gf_dim * 2],k_h=2, k_w=1,d_h=2, d_w=2 ,name='g_h3')))\n",
    "            h3 = conv_cond_concat(h3, yb)\n",
    "            h3 = conv_prev_concat(h3, h1_prev)\n",
    "\n",
    "            h4 = tf.nn.relu(self.g_bn4(deconv2d(h3, [self.batch_size, 16, 1, self.gf_dim * 2],k_h=2, k_w=1,d_h=2, d_w=2 ,name='g_h4')))\n",
    "            h4 = conv_cond_concat(h4, yb)\n",
    "            h4 = conv_prev_concat(h4, h0_prev)\n",
    "\n",
    "            return tf.nn.sigmoid(deconv2d(h4, [self.batch_size, 16, 128, self.c_dim],k_h=1, k_w=128,d_h=1, d_w=2, name='g_h5'))\n",
    "        \n",
    "    def save(self, checkpoint_dir, step):\n",
    "        model_name = \"MidiNet.model\"\n",
    "        model_dir = \"%s_%s_%s\" % (\"MIDIGAN\", self.batch_size, self.output_w)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,\n",
    "                        os.path.join(checkpoint_dir, model_name),\n",
    "                        global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer(\"epoch\", 2, \"Epoch to train [20]\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.00005, \"Learning rate of for adam [0.0002]\")\n",
    "flags.DEFINE_float(\"beta1\", 0.5, \"Momentum term of adam [0.5]\")\n",
    "flags.DEFINE_integer(\"batch_size\", 72, \"The size of batch [72]\")\n",
    "flags.DEFINE_integer(\"output_w\", 16, \"The size of the output segs to produce [16]\")\n",
    "flags.DEFINE_integer(\"output_h\", 128, \"The size of the output note to produce [128]\")\n",
    "flags.DEFINE_integer(\"c_dim\", 1, \"Number of Midi track. [1]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Directory name to save the checkpoints [checkpoint]\")\n",
    "flags.DEFINE_string(\"sample_dir\", \"samples\", \"Directory name to save the image samples [samples]\")\n",
    "flags.DEFINE_string(\"dataset\", \"MidiNet_v1\", \"The name of dataset \")\n",
    "flags.DEFINE_boolean(\"is_train\", True, \"True for training, False for testing [False]\")\n",
    "flags.DEFINE_boolean(\"is_crop\", False, \"True for training, False for testing [False]\")\n",
    "flags.DEFINE_boolean(\"generation_test\", False, \"True for generation_test, False for nothing [False]\")\n",
    "flags.DEFINE_string(\"gen_dir\", \"gen\", \"Directory name to save the generate samples [samples]\")\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 72,\n",
      " 'beta1': 0.5,\n",
      " 'c_dim': 1,\n",
      " 'checkpoint_dir': 'checkpoint',\n",
      " 'dataset': 'MidiNet_v1',\n",
      " 'epoch': 2,\n",
      " 'gen_dir': 'gen',\n",
      " 'generation_test': False,\n",
      " 'is_crop': False,\n",
      " 'is_train': True,\n",
      " 'learning_rate': 5e-05,\n",
      " 'output_h': 128,\n",
      " 'output_w': 16,\n",
      " 'sample_dir': 'samples'}\n",
      "GENERATOR\n",
      "DISCRIMINATOR\n",
      "SAMPLER\n",
      "Tensor(\"generator_1/Maximum:0\", shape=(72, 16, 1, 16), dtype=float32)\n",
      "DISCRIMINATOR\n",
      "Begin training...\n",
      "Starting epochs...\n",
      "Epoch: [ 0] [   0/   3] time: 0.9858, d_loss: 1.42000544, g_loss: 299.00064087\n",
      "[Sample] d_loss: 1.41767430, g_loss: 299.00888062\n",
      "Epoch: [ 0] [   1/   3] time: 1.6202, d_loss: 1.39558387, g_loss: 258.14575195\n",
      "Epoch: [ 0] [   2/   3] time: 2.1536, d_loss: 1.38155150, g_loss: 268.47909546\n",
      "Saving Checkpoint\n",
      "Epoch: [ 0] time: 2.6542, d_loss: 0.46051717\n",
      "Epoch: [ 1] [   0/   3] time: 3.3018, d_loss: 1.36324549, g_loss: 298.83688354\n",
      "Epoch: [ 1] [   1/   3] time: 3.8264, d_loss: 1.34504175, g_loss: 257.93237305\n",
      "Epoch: [ 1] [   2/   3] time: 4.4720, d_loss: 1.33482409, g_loss: 268.28787231\n",
      "Saving Checkpoint\n",
      "Epoch: [ 1] time: 4.9170, d_loss: 0.44494136\n",
      "[[[[ 0.50157952]\n",
      "   [ 0.52282202]\n",
      "   [ 0.47647136]\n",
      "   ..., \n",
      "   [ 0.41644603]\n",
      "   [ 0.46939322]\n",
      "   [ 0.50199407]]\n",
      "\n",
      "  [[ 0.45778951]\n",
      "   [ 0.46469593]\n",
      "   [ 0.45136017]\n",
      "   ..., \n",
      "   [ 0.47101364]\n",
      "   [ 0.5363009 ]\n",
      "   [ 0.45366976]]\n",
      "\n",
      "  [[ 0.54722583]\n",
      "   [ 0.47845685]\n",
      "   [ 0.59997946]\n",
      "   ..., \n",
      "   [ 0.418861  ]\n",
      "   [ 0.48889324]\n",
      "   [ 0.44656825]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.46723062]\n",
      "   [ 0.51760006]\n",
      "   [ 0.56360072]\n",
      "   ..., \n",
      "   [ 0.50112003]\n",
      "   [ 0.57147515]\n",
      "   [ 0.4901703 ]]\n",
      "\n",
      "  [[ 0.53616196]\n",
      "   [ 0.47541919]\n",
      "   [ 0.53243995]\n",
      "   ..., \n",
      "   [ 0.52183801]\n",
      "   [ 0.46257854]\n",
      "   [ 0.50742084]]\n",
      "\n",
      "  [[ 0.56719446]\n",
      "   [ 0.48125535]\n",
      "   [ 0.53385627]\n",
      "   ..., \n",
      "   [ 0.42999363]\n",
      "   [ 0.48385891]\n",
      "   [ 0.50012833]]]\n",
      "\n",
      "\n",
      " [[[ 0.50564021]\n",
      "   [ 0.48373148]\n",
      "   [ 0.49686515]\n",
      "   ..., \n",
      "   [ 0.49322632]\n",
      "   [ 0.51031399]\n",
      "   [ 0.51418668]]\n",
      "\n",
      "  [[ 0.49796003]\n",
      "   [ 0.50249267]\n",
      "   [ 0.51125062]\n",
      "   ..., \n",
      "   [ 0.53214067]\n",
      "   [ 0.53611612]\n",
      "   [ 0.47885618]]\n",
      "\n",
      "  [[ 0.48966417]\n",
      "   [ 0.53145617]\n",
      "   [ 0.5194419 ]\n",
      "   ..., \n",
      "   [ 0.47085792]\n",
      "   [ 0.50033975]\n",
      "   [ 0.48381639]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.52773398]\n",
      "   [ 0.44509473]\n",
      "   [ 0.52185458]\n",
      "   ..., \n",
      "   [ 0.52027124]\n",
      "   [ 0.55496049]\n",
      "   [ 0.54226989]]\n",
      "\n",
      "  [[ 0.48931813]\n",
      "   [ 0.47956935]\n",
      "   [ 0.51178181]\n",
      "   ..., \n",
      "   [ 0.51972735]\n",
      "   [ 0.49372482]\n",
      "   [ 0.49331194]]\n",
      "\n",
      "  [[ 0.51918173]\n",
      "   [ 0.4643696 ]\n",
      "   [ 0.52024686]\n",
      "   ..., \n",
      "   [ 0.51121223]\n",
      "   [ 0.50768781]\n",
      "   [ 0.51789111]]]\n",
      "\n",
      "\n",
      " [[[ 0.44000831]\n",
      "   [ 0.40130609]\n",
      "   [ 0.50677681]\n",
      "   ..., \n",
      "   [ 0.44324422]\n",
      "   [ 0.48666123]\n",
      "   [ 0.45394081]]\n",
      "\n",
      "  [[ 0.45912889]\n",
      "   [ 0.53182286]\n",
      "   [ 0.49840021]\n",
      "   ..., \n",
      "   [ 0.52307266]\n",
      "   [ 0.55306196]\n",
      "   [ 0.42449063]]\n",
      "\n",
      "  [[ 0.49289989]\n",
      "   [ 0.48655283]\n",
      "   [ 0.48958609]\n",
      "   ..., \n",
      "   [ 0.4743174 ]\n",
      "   [ 0.48487023]\n",
      "   [ 0.43114012]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.570903  ]\n",
      "   [ 0.44623038]\n",
      "   [ 0.49001032]\n",
      "   ..., \n",
      "   [ 0.44885087]\n",
      "   [ 0.54031128]\n",
      "   [ 0.55082476]]\n",
      "\n",
      "  [[ 0.55982357]\n",
      "   [ 0.50118566]\n",
      "   [ 0.44482899]\n",
      "   ..., \n",
      "   [ 0.46508515]\n",
      "   [ 0.49045098]\n",
      "   [ 0.44681334]]\n",
      "\n",
      "  [[ 0.50856018]\n",
      "   [ 0.47396329]\n",
      "   [ 0.52785087]\n",
      "   ..., \n",
      "   [ 0.47055757]\n",
      "   [ 0.44602329]\n",
      "   [ 0.5130583 ]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.54616398]\n",
      "   [ 0.48647028]\n",
      "   [ 0.46519658]\n",
      "   ..., \n",
      "   [ 0.44963986]\n",
      "   [ 0.51943207]\n",
      "   [ 0.51491004]]\n",
      "\n",
      "  [[ 0.52690309]\n",
      "   [ 0.41473472]\n",
      "   [ 0.49591768]\n",
      "   ..., \n",
      "   [ 0.53066832]\n",
      "   [ 0.53999519]\n",
      "   [ 0.48955569]]\n",
      "\n",
      "  [[ 0.52753925]\n",
      "   [ 0.49599355]\n",
      "   [ 0.49238023]\n",
      "   ..., \n",
      "   [ 0.51749128]\n",
      "   [ 0.52552575]\n",
      "   [ 0.46170199]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.47745013]\n",
      "   [ 0.49612814]\n",
      "   [ 0.46348092]\n",
      "   ..., \n",
      "   [ 0.53918993]\n",
      "   [ 0.56731385]\n",
      "   [ 0.50213915]]\n",
      "\n",
      "  [[ 0.5050661 ]\n",
      "   [ 0.42640689]\n",
      "   [ 0.49761119]\n",
      "   ..., \n",
      "   [ 0.4870235 ]\n",
      "   [ 0.46024233]\n",
      "   [ 0.46527976]]\n",
      "\n",
      "  [[ 0.51303715]\n",
      "   [ 0.52350271]\n",
      "   [ 0.50655669]\n",
      "   ..., \n",
      "   [ 0.49974751]\n",
      "   [ 0.52573484]\n",
      "   [ 0.47453597]]]\n",
      "\n",
      "\n",
      " [[[ 0.51053727]\n",
      "   [ 0.5133217 ]\n",
      "   [ 0.49117818]\n",
      "   ..., \n",
      "   [ 0.45926735]\n",
      "   [ 0.48424226]\n",
      "   [ 0.45796034]]\n",
      "\n",
      "  [[ 0.48863089]\n",
      "   [ 0.52803248]\n",
      "   [ 0.50105673]\n",
      "   ..., \n",
      "   [ 0.48772883]\n",
      "   [ 0.55184448]\n",
      "   [ 0.45215464]]\n",
      "\n",
      "  [[ 0.50479108]\n",
      "   [ 0.4961403 ]\n",
      "   [ 0.53159624]\n",
      "   ..., \n",
      "   [ 0.52014375]\n",
      "   [ 0.53893238]\n",
      "   [ 0.4373078 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.5402869 ]\n",
      "   [ 0.56204146]\n",
      "   [ 0.58910269]\n",
      "   ..., \n",
      "   [ 0.42203963]\n",
      "   [ 0.59246051]\n",
      "   [ 0.48849919]]\n",
      "\n",
      "  [[ 0.52244467]\n",
      "   [ 0.46760106]\n",
      "   [ 0.51351726]\n",
      "   ..., \n",
      "   [ 0.46947083]\n",
      "   [ 0.51041323]\n",
      "   [ 0.47920236]]\n",
      "\n",
      "  [[ 0.56392586]\n",
      "   [ 0.48747271]\n",
      "   [ 0.52898884]\n",
      "   ..., \n",
      "   [ 0.47540098]\n",
      "   [ 0.49587634]\n",
      "   [ 0.50825304]]]\n",
      "\n",
      "\n",
      " [[[ 0.51478577]\n",
      "   [ 0.46895915]\n",
      "   [ 0.47915533]\n",
      "   ..., \n",
      "   [ 0.48866272]\n",
      "   [ 0.53729385]\n",
      "   [ 0.46685389]]\n",
      "\n",
      "  [[ 0.47558564]\n",
      "   [ 0.51575619]\n",
      "   [ 0.48922125]\n",
      "   ..., \n",
      "   [ 0.46302244]\n",
      "   [ 0.52011782]\n",
      "   [ 0.45276895]]\n",
      "\n",
      "  [[ 0.51197809]\n",
      "   [ 0.50551254]\n",
      "   [ 0.50108606]\n",
      "   ..., \n",
      "   [ 0.4816474 ]\n",
      "   [ 0.50450504]\n",
      "   [ 0.46335647]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.46739218]\n",
      "   [ 0.42334032]\n",
      "   [ 0.56391805]\n",
      "   ..., \n",
      "   [ 0.48946649]\n",
      "   [ 0.52672303]\n",
      "   [ 0.51463169]]\n",
      "\n",
      "  [[ 0.55895758]\n",
      "   [ 0.44139674]\n",
      "   [ 0.53153157]\n",
      "   ..., \n",
      "   [ 0.46198159]\n",
      "   [ 0.46263537]\n",
      "   [ 0.49938107]]\n",
      "\n",
      "  [[ 0.56686485]\n",
      "   [ 0.50086772]\n",
      "   [ 0.52226341]\n",
      "   ..., \n",
      "   [ 0.4930627 ]\n",
      "   [ 0.4927873 ]\n",
      "   [ 0.51154482]]]]\n",
      "[[ 0.50157952  0.50564021  0.44000831 ...,  0.51303715  0.56392586\n",
      "   0.56686485]\n",
      " [ 0.52282202  0.48373148  0.40130609 ...,  0.52350271  0.48747271\n",
      "   0.50086772]\n",
      " [ 0.47647136  0.49686515  0.50677681 ...,  0.50655669  0.52898884\n",
      "   0.52226341]\n",
      " ..., \n",
      " [ 0.41644603  0.49322632  0.44324422 ...,  0.49974751  0.47540098\n",
      "   0.4930627 ]\n",
      " [ 0.46939322  0.51031399  0.48666123 ...,  0.52573484  0.49587634\n",
      "   0.4927873 ]\n",
      " [ 0.50199407  0.51418668  0.45394081 ...,  0.47453597  0.50825304\n",
      "   0.51154482]]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbnahata/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(_):\n",
    "    pp.pprint(flags.FLAGS.__flags)\n",
    "\n",
    "    if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "        os.makedirs(FLAGS.checkpoint_dir)\n",
    "    if not os.path.exists(FLAGS.sample_dir):\n",
    "        os.makedirs(FLAGS.sample_dir)\n",
    "    if not os.path.exists(FLAGS.gen_dir):\n",
    "        os.makedirs(FLAGS.gen_dir)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        if FLAGS.dataset == 'MidiNet_v1':\n",
    "            model = MidiNet(sess,  batch_size=FLAGS.batch_size,y_dim=13, output_w=FLAGS.output_w, \n",
    "                            output_h=FLAGS.output_h, c_dim=FLAGS.c_dim)\n",
    "        \n",
    "        if FLAGS.is_train:\n",
    "            model.train(FLAGS)\n",
    "            \n",
    "        k = generation_test (sess, model, FLAGS, 0)\n",
    "        print(k)\n",
    "        k = np.transpose(k,(2,1,0,3))\n",
    "        k = np.reshape(k,[128, -1])\n",
    "        print(k)\n",
    "        pm = piano_roll_to_pretty_midi(k)\n",
    "        pm.write('/Users/sbnahata/Documents/University of Michigan/EECS 545/Project_local/FINALLY.mid')\n",
    "#         else:\n",
    "#             model.load(FLAGS.checkpoint_dir)\n",
    "\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
